Dataset: ETTh1
Arguments: Namespace(alpha=0.0005, archive='forecast_csv', batch_size=128, dataset='ETTh1', epochs=None, eval=True, gpu=0, iters=None, kernels=[1, 2, 4, 8, 16, 32, 64, 128], lr=0.001, max_threads=8, max_train_length=201, repr_dims=320, run_name='forecast_multivar', save_every=None, seed=0)
input_fc.weight	[14, 64]	Place(gpu:0)
input_fc.bias	[64]	Place(gpu:0)
feature_extractor.net.0.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.0.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.0.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.0.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.1.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.1.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.1.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.1.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.2.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.2.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.2.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.2.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.3.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.3.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.3.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.3.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.4.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.4.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.4.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.4.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.5.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.5.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.5.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.5.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.6.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.6.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.6.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.6.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.7.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.7.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.7.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.7.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.8.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.8.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.8.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.8.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.9.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.9.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.9.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.9.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.10.conv1.conv.weight	[320, 64, 3]	Place(gpu:0)
feature_extractor.net.10.conv1.conv.bias	[320]	Place(gpu:0)
feature_extractor.net.10.conv2.conv.weight	[320, 320, 3]	Place(gpu:0)
feature_extractor.net.10.conv2.conv.bias	[320]	Place(gpu:0)
feature_extractor.net.10.projector.weight	[320, 64, 1]	Place(gpu:0)
feature_extractor.net.10.projector.bias	[320]	Place(gpu:0)
tfd.0.weight	[160, 320, 1]	Place(gpu:0)
tfd.0.bias	[160]	Place(gpu:0)
tfd.1.weight	[160, 320, 2]	Place(gpu:0)
tfd.1.bias	[160]	Place(gpu:0)
tfd.2.weight	[160, 320, 4]	Place(gpu:0)
tfd.2.bias	[160]	Place(gpu:0)
tfd.3.weight	[160, 320, 8]	Place(gpu:0)
tfd.3.bias	[160]	Place(gpu:0)
tfd.4.weight	[160, 320, 16]	Place(gpu:0)
tfd.4.bias	[160]	Place(gpu:0)
tfd.5.weight	[160, 320, 32]	Place(gpu:0)
tfd.5.bias	[160]	Place(gpu:0)
tfd.6.weight	[160, 320, 64]	Place(gpu:0)
tfd.6.bias	[160]	Place(gpu:0)
tfd.7.weight	[160, 320, 128]	Place(gpu:0)
tfd.7.bias	[160]	Place(gpu:0)
sfd.0.weight	[101, 320, 160]	Place(gpu:0)
sfd.0.bias	[101, 160]	Place(gpu:0)
---------------------------------------------------------------
input_fc.weight	[14, 64]	Place(gpu:0)
input_fc.bias	[64]	Place(gpu:0)
feature_extractor.net.0.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.0.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.0.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.0.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.1.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.1.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.1.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.1.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.2.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.2.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.2.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.2.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.3.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.3.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.3.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.3.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.4.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.4.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.4.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.4.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.5.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.5.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.5.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.5.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.6.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.6.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.6.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.6.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.7.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.7.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.7.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.7.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.8.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.8.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.8.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.8.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.9.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.9.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.9.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.9.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.10.conv1.conv.weight	[320, 64, 3]	Place(gpu:0)
feature_extractor.net.10.conv1.conv.bias	[320]	Place(gpu:0)
feature_extractor.net.10.conv2.conv.weight	[320, 320, 3]	Place(gpu:0)
feature_extractor.net.10.conv2.conv.bias	[320]	Place(gpu:0)
feature_extractor.net.10.projector.weight	[320, 64, 1]	Place(gpu:0)
feature_extractor.net.10.projector.bias	[320]	Place(gpu:0)
tfd.0.weight	[160, 320, 1]	Place(gpu:0)
tfd.0.bias	[160]	Place(gpu:0)
tfd.1.weight	[160, 320, 2]	Place(gpu:0)
tfd.1.bias	[160]	Place(gpu:0)
tfd.2.weight	[160, 320, 4]	Place(gpu:0)
tfd.2.bias	[160]	Place(gpu:0)
tfd.3.weight	[160, 320, 8]	Place(gpu:0)
tfd.3.bias	[160]	Place(gpu:0)
tfd.4.weight	[160, 320, 16]	Place(gpu:0)
tfd.4.bias	[160]	Place(gpu:0)
tfd.5.weight	[160, 320, 32]	Place(gpu:0)
tfd.5.bias	[160]	Place(gpu:0)
tfd.6.weight	[160, 320, 64]	Place(gpu:0)
tfd.6.bias	[160]	Place(gpu:0)
tfd.7.weight	[160, 320, 128]	Place(gpu:0)
tfd.7.bias	[160]	Place(gpu:0)
sfd.0.weight	[101, 320, 160]	Place(gpu:0)
sfd.0.bias	[101, 160]	Place(gpu:0)
Epoch #0: loss=0.09111425280570984
Epoch #1: loss=2.8691141605377197
Epoch #2: loss=2.083372116088867
Epoch #3: loss=3.277621030807495
Epoch #4: loss=3.0168843269348145
Epoch #5: loss=2.0406439304351807
Epoch #6: loss=0.49195176362991333
Epoch #7: loss=3.0788066387176514
Epoch #8: loss=3.4395735263824463
Epoch #9: loss=3.126086473464966
Epoch #10: loss=3.3078372478485107
Epoch #11: loss=3.280325412750244
Epoch #12: loss=3.0605826377868652
Epoch #13: loss=4.142418384552002
Epoch #14: loss=2.742614984512329
Epoch #15: loss=3.512704610824585
Epoch #16: loss=2.7894647121429443
Epoch #17: loss=3.245826244354248
Epoch #18: loss=1.2596560716629028
Epoch #19: loss=1.5481983423233032
Epoch #20: loss=3.6840829849243164
Epoch #21: loss=3.7506349086761475
Epoch #22: loss=1.5797889232635498
Epoch #23: loss=3.3448026180267334
Epoch #24: loss=2.109933853149414
Epoch #25: loss=2.7903077602386475
Epoch #26: loss=0.39068901538848877
Epoch #27: loss=4.168085098266602
Epoch #28: loss=2.4162232875823975
Epoch #29: loss=4.961215019226074
Epoch #30: loss=4.404392719268799
Epoch #31: loss=1.599875807762146
Epoch #32: loss=3.855240821838379
Epoch #33: loss=1.4725024700164795
Epoch #34: loss=2.9297449588775635
Epoch #35: loss=2.8601760864257812
Epoch #36: loss=3.111814498901367
Epoch #37: loss=1.6526697874069214
Epoch #38: loss=2.9545788764953613
Epoch #39: loss=3.246351480484009
Epoch #40: loss=2.7546966075897217
Epoch #41: loss=3.290530204772949
Epoch #42: loss=3.283722162246704
Epoch #43: loss=2.8420872688293457
Epoch #44: loss=1.366636037826538
Epoch #45: loss=3.623574733734131
Epoch #46: loss=2.8842620849609375
Epoch #47: loss=2.6538796424865723
Epoch #48: loss=0.5925517678260803
Epoch #49: loss=2.43076229095459
Epoch #50: loss=3.1624419689178467
Epoch #51: loss=2.6967852115631104
Epoch #52: loss=1.6282713413238525
Epoch #53: loss=1.9500163793563843
Epoch #54: loss=2.8654000759124756
Epoch #55: loss=3.3310611248016357
Epoch #56: loss=1.334876298904419
Epoch #57: loss=2.3147904872894287
Epoch #58: loss=2.6439006328582764
Epoch #59: loss=2.764448404312134
Epoch #60: loss=1.2764679193496704
Epoch #61: loss=2.607144355773926
Epoch #62: loss=0.9391728043556213
Epoch #63: loss=2.9310693740844727
Epoch #64: loss=0.9385104179382324
Epoch #65: loss=2.2236328125
Epoch #66: loss=2.820805311203003
Epoch #67: loss=2.638841152191162
Epoch #68: loss=0.5777603983879089
Epoch #69: loss=2.3902623653411865
Epoch #70: loss=1.929857850074768
Epoch #71: loss=0.947801947593689
Epoch #72: loss=2.555757761001587
Epoch #73: loss=3.0610814094543457
Epoch #74: loss=2.8999714851379395
Epoch #75: loss=2.406397819519043
Epoch #76: loss=2.9204978942871094
Epoch #77: loss=2.3311429023742676
Epoch #78: loss=1.6879485845565796
Epoch #79: loss=2.7238073348999023
Epoch #80: loss=2.8018600940704346
Epoch #81: loss=0.35898175835609436
Epoch #82: loss=2.6909339427948
Epoch #83: loss=1.9859949350357056
Epoch #84: loss=2.512671709060669
Epoch #85: loss=2.9951772689819336
Epoch #86: loss=1.6365909576416016
Epoch #87: loss=2.3709938526153564
Epoch #88: loss=0.37241148948669434
Epoch #89: loss=2.918191909790039
Epoch #90: loss=2.292848825454712
Epoch #91: loss=0.30936112999916077
Epoch #92: loss=1.8904955387115479
Epoch #93: loss=1.8019124269485474
Epoch #94: loss=3.1220943927764893
Epoch #95: loss=2.747682809829712
Epoch #96: loss=1.5725566148757935
Epoch #97: loss=2.561310052871704
Epoch #98: loss=1.9251378774642944
Epoch #99: loss=1.3752940893173218
Epoch #100: loss=1.850577712059021
Epoch #101: loss=0.9701893329620361
Epoch #102: loss=2.4131996631622314
Epoch #103: loss=1.7661441564559937
Epoch #104: loss=1.9325846433639526
Epoch #105: loss=0.5348603129386902
Epoch #106: loss=3.1510708332061768
Epoch #107: loss=2.0542612075805664
Epoch #108: loss=0.4018405079841614
Epoch #109: loss=2.8796849250793457
Epoch #110: loss=1.979877233505249
Epoch #111: loss=1.1571059226989746
Epoch #112: loss=1.3688604831695557
Epoch #113: loss=1.5067486763000488
Epoch #114: loss=1.5656039714813232
Epoch #115: loss=2.028669834136963
Epoch #116: loss=2.0880990028381348
Epoch #117: loss=0.8210785984992981
Epoch #118: loss=2.8849129676818848
Epoch #119: loss=1.7507858276367188
Epoch #120: loss=1.1782922744750977
Epoch #121: loss=2.6852521896362305
Epoch #122: loss=2.3905630111694336
Epoch #123: loss=1.4049745798110962
Epoch #124: loss=0.3098100423812866
Epoch #125: loss=0.967423677444458
Epoch #126: loss=1.9294534921646118
Epoch #127: loss=2.551966667175293
Epoch #128: loss=1.860701084136963
Epoch #129: loss=2.8927369117736816
Epoch #130: loss=1.6257636547088623
Epoch #131: loss=1.08293616771698
Epoch #132: loss=2.541367292404175
Epoch #133: loss=1.933260440826416
Epoch #134: loss=2.0057129859924316
Epoch #135: loss=0.9607201814651489
Epoch #136: loss=2.0358681678771973
Epoch #137: loss=0.21684743463993073
Epoch #138: loss=2.601250410079956
Epoch #139: loss=1.5085299015045166
Epoch #140: loss=0.6086783409118652
Epoch #141: loss=2.8882076740264893
Epoch #142: loss=2.5763437747955322
Epoch #143: loss=0.8035719394683838
Epoch #144: loss=1.3590073585510254
Epoch #145: loss=0.901897668838501
Epoch #146: loss=2.404491424560547
Epoch #147: loss=0.5276321768760681
Epoch #148: loss=1.343646764755249
Epoch #149: loss=2.065613031387329
Epoch #150: loss=1.813981294631958
Epoch #151: loss=0.22791142761707306
Epoch #152: loss=2.4747555255889893
Epoch #153: loss=1.2314565181732178
Epoch #154: loss=0.5915427803993225
Epoch #155: loss=2.1398532390594482
Epoch #156: loss=0.19230759143829346
Epoch #157: loss=2.513679265975952
Epoch #158: loss=2.182046413421631
Epoch #159: loss=2.8799400329589844
Epoch #160: loss=1.5597753524780273
Epoch #161: loss=0.18603692948818207
Epoch #162: loss=3.4821043014526367
Epoch #163: loss=1.206541895866394
Epoch #164: loss=2.593142509460449
Epoch #165: loss=1.9466334581375122
Epoch #166: loss=0.7960483431816101
Epoch #167: loss=2.4534213542938232
Epoch #168: loss=1.8717825412750244
Epoch #169: loss=1.1122275590896606
Epoch #170: loss=1.345219612121582
Epoch #171: loss=2.105729579925537
Epoch #172: loss=0.7720990180969238
Epoch #173: loss=1.679100751876831
Epoch #174: loss=0.31413534283638
Epoch #175: loss=1.1254888772964478
Epoch #176: loss=1.3844338655471802
Epoch #177: loss=2.7073252201080322
Epoch #178: loss=2.3364856243133545
Epoch #179: loss=0.681836724281311
Epoch #180: loss=2.188474655151367
Epoch #181: loss=1.6459177732467651
Epoch #182: loss=0.8406949639320374
Epoch #183: loss=0.8135809898376465
Epoch #184: loss=2.1394031047821045
Epoch #185: loss=1.0715012550354004
Epoch #186: loss=0.20732775330543518
Epoch #187: loss=0.42592519521713257
Epoch #188: loss=1.6875394582748413
Epoch #189: loss=0.18457743525505066
Epoch #190: loss=1.390702724456787
Epoch #191: loss=0.6036196947097778
Epoch #192: loss=1.3028388023376465
Epoch #193: loss=2.2449753284454346
Epoch #194: loss=0.25809553265571594
Epoch #195: loss=0.9384673237800598
Epoch #196: loss=1.2085583209991455
Epoch #197: loss=0.15909677743911743
Epoch #198: loss=1.4774240255355835
Epoch #199: loss=2.993856906890869
Epoch #200: loss=1.6095588207244873
Epoch #201: loss=0.7126573920249939
Epoch #202: loss=1.865011215209961
Epoch #203: loss=1.2050471305847168
Epoch #204: loss=2.3047549724578857
Epoch #205: loss=2.6303462982177734
Epoch #206: loss=1.1354222297668457
Epoch #207: loss=1.2355009317398071
Epoch #208: loss=1.5125001668930054
Epoch #209: loss=1.7213644981384277
Epoch #210: loss=0.2566981017589569
Epoch #211: loss=0.9144943952560425
Epoch #212: loss=0.5890841484069824
Epoch #213: loss=1.6019794940948486
Epoch #214: loss=0.4071950614452362
Epoch #215: loss=0.21256153285503387
Epoch #216: loss=0.46447017788887024
Epoch #217: loss=0.847535252571106
Epoch #218: loss=0.3173595368862152
Epoch #219: loss=2.035677194595337
Epoch #220: loss=1.4657407999038696
Epoch #221: loss=0.2819652259349823
Epoch #222: loss=1.796709418296814
Epoch #223: loss=1.6974245309829712
Epoch #224: loss=2.1417956352233887
Epoch #225: loss=2.3179121017456055
Epoch #226: loss=0.8535065054893494
Epoch #227: loss=1.9375617504119873
Epoch #228: loss=0.9095024466514587
Epoch #229: loss=0.06460131704807281
Epoch #230: loss=0.6540948152542114
Epoch #231: loss=1.7740761041641235
Epoch #232: loss=0.24993574619293213
Epoch #233: loss=2.1773359775543213
Epoch #234: loss=1.7288787364959717
Epoch #235: loss=2.474510908126831
Epoch #236: loss=0.40895190834999084
Epoch #237: loss=0.8975251913070679
Epoch #238: loss=1.9124987125396729
Epoch #239: loss=0.31595349311828613
Epoch #240: loss=1.6041923761367798
Epoch #241: loss=0.39015093445777893
Epoch #242: loss=1.5581055879592896
Epoch #243: loss=1.6563081741333008
Epoch #244: loss=0.16322439908981323
Epoch #245: loss=1.2207903861999512
Epoch #246: loss=2.9476864337921143
Epoch #247: loss=1.7184042930603027
Epoch #248: loss=0.8880789279937744
Epoch #249: loss=2.464463233947754
Epoch #250: loss=1.4339537620544434
Epoch #251: loss=2.343587636947632
Epoch #252: loss=0.2167087346315384
Epoch #253: loss=2.0566253662109375
Epoch #254: loss=0.6035404205322266
Epoch #255: loss=0.5820797681808472
Epoch #256: loss=0.9254056215286255
Epoch #257: loss=1.5043821334838867
Epoch #258: loss=0.3230787515640259
Epoch #259: loss=1.2766642570495605
Epoch #260: loss=0.7327322959899902
Epoch #261: loss=1.6344150304794312
Epoch #262: loss=0.6551443934440613
Epoch #263: loss=1.8504363298416138
Epoch #264: loss=0.34357985854148865
Epoch #265: loss=1.4138588905334473
Epoch #266: loss=3.0337491035461426
Epoch #267: loss=0.6486119627952576
Epoch #268: loss=0.281491219997406
Epoch #269: loss=0.3460187613964081
Epoch #270: loss=0.5101308226585388
Epoch #271: loss=2.269306182861328
Epoch #272: loss=0.1753797084093094
Epoch #273: loss=1.742501139640808
Epoch #274: loss=2.3103134632110596
Epoch #275: loss=1.7549219131469727
Epoch #276: loss=1.3660094738006592
Epoch #277: loss=1.8996219635009766
Epoch #278: loss=2.0277163982391357
Epoch #279: loss=0.13503405451774597
Epoch #280: loss=0.2750321626663208
Epoch #281: loss=1.7827608585357666
Epoch #282: loss=0.9657294154167175
Epoch #283: loss=0.14420299232006073
Epoch #284: loss=0.43025508522987366
Epoch #285: loss=0.9684625267982483
Epoch #286: loss=0.9869248867034912
Epoch #287: loss=2.1642005443573
Epoch #288: loss=2.8372819423675537
Epoch #289: loss=0.9100130796432495
Epoch #290: loss=1.8948607444763184
Epoch #291: loss=0.7979612946510315
Epoch #292: loss=0.407267302274704
Epoch #293: loss=0.2789399027824402
Epoch #294: loss=0.594750165939331
Epoch #295: loss=0.31813234090805054
Epoch #296: loss=0.4587000608444214
Epoch #297: loss=1.549623966217041
Epoch #298: loss=0.22785796225070953
Epoch #299: loss=1.267681360244751
Epoch #300: loss=0.8269481062889099
Epoch #301: loss=2.281564235687256
Epoch #302: loss=2.1767125129699707
Epoch #303: loss=0.29364442825317383
Epoch #304: loss=0.7147272229194641
Epoch #305: loss=0.475722074508667
Epoch #306: loss=1.8736704587936401
Epoch #307: loss=0.539405107498169
Epoch #308: loss=0.421143114566803
Epoch #309: loss=1.8332371711730957
Epoch #310: loss=0.5181759595870972
Epoch #311: loss=1.6222501993179321
Epoch #312: loss=0.5307199954986572
Epoch #313: loss=0.20246522128582
Epoch #314: loss=1.8897727727890015
Epoch #315: loss=1.4566409587860107
Epoch #316: loss=0.9470361471176147
Epoch #317: loss=2.2462244033813477
Epoch #318: loss=0.29310765862464905
Epoch #319: loss=0.08955278992652893
Epoch #320: loss=0.4041380286216736
Epoch #321: loss=1.5549452304840088
Epoch #322: loss=0.4232368767261505
Epoch #323: loss=1.9662050008773804
Epoch #324: loss=2.026962995529175
Epoch #325: loss=0.3512367904186249
Epoch #326: loss=1.8779866695404053
Epoch #327: loss=0.9194939136505127
Epoch #328: loss=0.24815647304058075
Epoch #329: loss=1.857763409614563
Epoch #330: loss=0.12801767885684967
Epoch #331: loss=0.7122572064399719
Epoch #332: loss=0.4121319055557251
Epoch #333: loss=0.1657179594039917
Epoch #334: loss=1.3411868810653687
Epoch #335: loss=2.7996017932891846
Epoch #336: loss=0.3105979561805725
Epoch #337: loss=0.5005200505256653
Epoch #338: loss=0.5530526638031006
Epoch #339: loss=1.3747819662094116
Epoch #340: loss=1.3852190971374512
Epoch #341: loss=0.1420324146747589
Epoch #342: loss=0.9886403679847717
Epoch #343: loss=0.3591509759426117
Epoch #344: loss=0.14386731386184692
Epoch #345: loss=0.39427056908607483
Epoch #346: loss=0.45581701397895813
Epoch #347: loss=1.8101803064346313
Epoch #348: loss=1.5546438694000244
Epoch #349: loss=0.5318559408187866
Epoch #350: loss=0.7634114027023315
Epoch #351: loss=0.7869696617126465
Epoch #352: loss=0.1881377100944519
Epoch #353: loss=0.13070739805698395
Epoch #354: loss=0.07656843215227127
Epoch #355: loss=2.6800763607025146
Epoch #356: loss=0.8125490546226501
Epoch #357: loss=0.16149428486824036
Epoch #358: loss=0.8381452560424805
Epoch #359: loss=1.0297962427139282
Epoch #360: loss=0.36076676845550537
Epoch #361: loss=1.3948079347610474
Epoch #362: loss=0.5667166113853455
Epoch #363: loss=1.571798324584961
Epoch #364: loss=1.8977797031402588
Epoch #365: loss=0.25602826476097107
Epoch #366: loss=0.4749661087989807
Epoch #367: loss=0.21793435513973236
Epoch #368: loss=1.83305823802948
Epoch #369: loss=0.152022123336792
Epoch #370: loss=1.8511078357696533
Epoch #371: loss=0.16700899600982666
Epoch #372: loss=0.6117182970046997
Epoch #373: loss=1.2634512186050415
Epoch #374: loss=1.5333821773529053
Epoch #375: loss=0.4898683428764343
Epoch #376: loss=1.4002727270126343
Epoch #377: loss=0.3089269995689392
Epoch #378: loss=0.2525637447834015
Epoch #379: loss=0.9112695455551147
Epoch #380: loss=0.37020739912986755
Epoch #381: loss=1.561721920967102
Epoch #382: loss=1.8956084251403809
Epoch #383: loss=0.13136976957321167
Epoch #384: loss=0.35678476095199585
Epoch #385: loss=1.0575536489486694
Epoch #386: loss=2.234762191772461
Epoch #387: loss=1.8859763145446777
Epoch #388: loss=2.1435632705688477
Epoch #389: loss=0.07174868881702423
Epoch #390: loss=1.074748158454895
Epoch #391: loss=0.7110905051231384
Epoch #392: loss=0.11850021779537201
Epoch #393: loss=2.0871171951293945
Epoch #394: loss=0.6785221099853516
Epoch #395: loss=0.3400878310203552
Epoch #396: loss=1.4816581010818481
Epoch #397: loss=0.11094460636377335
Epoch #398: loss=1.2424418926239014
Epoch #399: loss=1.4763576984405518
Epoch #400: loss=0.3789987862110138
Epoch #401: loss=1.4704704284667969
Epoch #402: loss=0.6541924476623535
Epoch #403: loss=0.28912636637687683
Epoch #404: loss=0.3377803564071655
Epoch #405: loss=0.16367194056510925
Epoch #406: loss=2.4543027877807617
Epoch #407: loss=0.15767869353294373
Epoch #408: loss=0.30323851108551025
Epoch #409: loss=0.5279679298400879
Epoch #410: loss=1.2090271711349487
Epoch #411: loss=0.0924176499247551
Epoch #412: loss=1.098695158958435
Epoch #413: loss=1.506590723991394
Epoch #414: loss=0.08520447462797165
Epoch #415: loss=0.5030895471572876
Epoch #416: loss=1.7879304885864258
Epoch #417: loss=0.21568650007247925
Epoch #418: loss=0.320415198802948
Epoch #419: loss=1.8248441219329834
Epoch #420: loss=2.2942399978637695
Epoch #421: loss=0.4196263253688812
Epoch #422: loss=0.19703568518161774
Epoch #423: loss=0.5401408076286316
Epoch #424: loss=0.5140830278396606
Epoch #425: loss=1.8149387836456299
Epoch #426: loss=1.7494431734085083
Epoch #427: loss=2.4270195960998535
Epoch #428: loss=0.8476335406303406
Epoch #429: loss=1.6283448934555054
Epoch #430: loss=1.6591260433197021
Epoch #431: loss=1.052864909172058
Epoch #432: loss=0.11138763278722763
Epoch #433: loss=0.21038000285625458
Epoch #434: loss=2.2257256507873535
Epoch #435: loss=0.4023648798465729
Epoch #436: loss=0.10641679167747498
Epoch #437: loss=3.3672454357147217
Epoch #438: loss=3.710552215576172
Epoch #439: loss=0.11378413438796997
Epoch #440: loss=1.7283804416656494
Epoch #441: loss=1.4820828437805176
Epoch #442: loss=1.590983271598816
Epoch #443: loss=1.349128007888794
Epoch #444: loss=1.8179763555526733
Epoch #445: loss=0.75531405210495
Epoch #446: loss=1.085890531539917
Epoch #447: loss=0.29359757900238037
Epoch #448: loss=0.7988812923431396
Epoch #449: loss=0.6155299544334412
Epoch #450: loss=0.9185734987258911
Epoch #451: loss=1.7599767446517944
Epoch #452: loss=0.42491453886032104
Epoch #453: loss=0.22349315881729126
Epoch #454: loss=0.6483134031295776
Epoch #455: loss=0.4272352457046509
Epoch #456: loss=0.10839521884918213
Epoch #457: loss=0.30074331164360046
Epoch #458: loss=1.8364195823669434
Epoch #459: loss=0.23057715594768524
Epoch #460: loss=0.6182584762573242
Epoch #461: loss=1.227605938911438
Epoch #462: loss=0.24666038155555725
Epoch #463: loss=1.034490704536438
Epoch #464: loss=0.3146103322505951
Epoch #465: loss=0.07196919620037079
Epoch #466: loss=0.2863925099372864
Epoch #467: loss=1.0465977191925049
Epoch #468: loss=0.28704503178596497
Epoch #469: loss=1.4844039678573608
Epoch #470: loss=0.14873594045639038
Epoch #471: loss=0.4362087547779083
Epoch #472: loss=0.7159433364868164
Epoch #473: loss=0.16145847737789154
Epoch #474: loss=0.3323346972465515
Epoch #475: loss=0.4644119441509247
Epoch #476: loss=1.290858268737793
Epoch #477: loss=0.36944448947906494
Epoch #478: loss=0.8566645383834839
Epoch #479: loss=0.5330311059951782
Epoch #480: loss=1.426927924156189
Epoch #481: loss=0.6670281887054443
Epoch #482: loss=1.5495120286941528
Epoch #483: loss=0.25497597455978394
Epoch #484: loss=1.844478726387024
Epoch #485: loss=0.1516546607017517
Epoch #486: loss=1.8455555438995361
Epoch #487: loss=0.29361361265182495
Epoch #488: loss=2.034301280975342
Epoch #489: loss=0.17551353573799133
Epoch #490: loss=1.6362613439559937
Epoch #491: loss=0.10564213246107101
Epoch #492: loss=0.1623721718788147
Epoch #493: loss=1.5093203783035278
Epoch #494: loss=1.4460903406143188
Epoch #495: loss=1.2748160362243652
Epoch #496: loss=0.2859782874584198
Epoch #497: loss=0.38636037707328796
Epoch #498: loss=1.5152920484542847
Epoch #499: loss=1.397377371788025
Epoch #500: loss=0.1694691777229309
Epoch #501: loss=0.351042240858078
Epoch #502: loss=0.14991268515586853
Epoch #503: loss=1.6031996011734009
Epoch #504: loss=1.4891612529754639
Epoch #505: loss=0.17222560942173004
Epoch #506: loss=2.019444704055786
Epoch #507: loss=0.35872405767440796
Epoch #508: loss=1.484838604927063
Epoch #509: loss=1.5404322147369385
Epoch #510: loss=0.15163502097129822
Epoch #511: loss=1.0892196893692017
Epoch #512: loss=0.29840198159217834
Epoch #513: loss=0.24310889840126038
Epoch #514: loss=1.6251174211502075
Epoch #515: loss=0.12575948238372803
Epoch #516: loss=0.8005720973014832
Epoch #517: loss=1.75771164894104
Epoch #518: loss=0.3810301125049591
Epoch #519: loss=0.3282615542411804
Epoch #520: loss=1.3336328268051147
Epoch #521: loss=0.7208656072616577
Epoch #522: loss=0.3675725758075714
Epoch #523: loss=1.8528059720993042
Epoch #524: loss=1.3931912183761597
Epoch #525: loss=0.5536489486694336
Epoch #526: loss=0.15418536961078644
Epoch #527: loss=1.6382719278335571
Epoch #528: loss=0.875055730342865
Epoch #529: loss=2.343893527984619
Epoch #530: loss=0.09188349545001984
Epoch #531: loss=2.097029447555542
Epoch #532: loss=0.19687160849571228
Epoch #533: loss=0.3062199056148529
Epoch #534: loss=2.0958709716796875
Epoch #535: loss=0.32636699080467224
Epoch #536: loss=2.1331639289855957
Epoch #537: loss=0.8831706047058105
Epoch #538: loss=0.5683028697967529
Epoch #539: loss=1.3999990224838257
Epoch #540: loss=0.054359398782253265
Epoch #541: loss=0.2002534717321396
Epoch #542: loss=0.11584816873073578
Epoch #543: loss=1.3021857738494873
Epoch #544: loss=0.21184641122817993
Epoch #545: loss=0.9179627299308777
Epoch #546: loss=0.7943434715270996
Epoch #547: loss=1.4534341096878052
Epoch #548: loss=0.21685576438903809
Epoch #549: loss=1.804957389831543
Epoch #550: loss=2.284461498260498
Epoch #551: loss=1.7182847261428833
Epoch #552: loss=0.1203448474407196
Epoch #553: loss=0.9660184383392334
Epoch #554: loss=0.09705537557601929
Epoch #555: loss=0.2338482141494751
Epoch #556: loss=1.6553668975830078
Epoch #557: loss=0.3037290871143341
Epoch #558: loss=1.1282017230987549
Epoch #559: loss=0.8381887078285217
Epoch #560: loss=1.421316146850586
Epoch #561: loss=1.778821349143982
Epoch #562: loss=1.0033754110336304
Epoch #563: loss=0.9240103960037231
Epoch #564: loss=1.1648085117340088
Epoch #565: loss=1.7783876657485962
Epoch #566: loss=0.5128293037414551
Epoch #567: loss=1.6350429058074951
Epoch #568: loss=0.2389991134405136
Epoch #569: loss=0.6070981621742249
Epoch #570: loss=0.11665422469377518
Epoch #571: loss=1.7566410303115845
Epoch #572: loss=0.12478676438331604
Epoch #573: loss=1.0439121723175049
Epoch #574: loss=0.05703584849834442
Epoch #575: loss=1.7145496606826782
Epoch #576: loss=0.7712246179580688
Epoch #577: loss=0.358727365732193
Epoch #578: loss=1.2102539539337158
Epoch #579: loss=0.40819376707077026
Epoch #580: loss=1.4205913543701172
Epoch #581: loss=0.1705171763896942
Epoch #582: loss=0.6758912205696106
Epoch #583: loss=0.09855835139751434
Epoch #584: loss=0.22721177339553833
Epoch #585: loss=0.19271118938922882
Epoch #586: loss=0.42733317613601685
Epoch #587: loss=0.6790026426315308
Epoch #588: loss=0.14023324847221375
Epoch #589: loss=0.16466084122657776
Epoch #590: loss=0.21996325254440308
Epoch #591: loss=1.0743025541305542
Epoch #592: loss=0.27322351932525635
Epoch #593: loss=0.1624365746974945
Epoch #594: loss=0.7956072688102722
Epoch #595: loss=0.07555535435676575
Epoch #596: loss=2.2748093605041504
Epoch #597: loss=2.032555103302002
Epoch #598: loss=0.05262380465865135
Epoch #599: loss=0.3489914834499359

Training time: 0:06:46.139698

Evaluation result: {'ours': {24: {'norm': {'MSE': 0.6712022097597823, 'MAE': 0.6298785035442218}, 'raw': {'MSE': 14.672419055347461, 'MAE': 2.411799145891094}}, 48: {'norm': {'MSE': 0.7119976694652735, 'MAE': 0.649215917576891}, 'raw': {'MSE': 15.703313804830877, 'MAE': 2.492265101674151}}, 168: {'norm': {'MSE': 0.8421198260449353, 'MAE': 0.7075989958089053}, 'raw': {'MSE': 17.201471636191393, 'MAE': 2.6339053946168525}}, 336: {'norm': {'MSE': 0.9783082661529101, 'MAE': 0.7703271602675434}, 'raw': {'MSE': 18.03709250905952, 'MAE': 2.7503146680246275}}, 720: {'norm': {'MSE': 1.1202768126045624, 'MAE': 0.8425831442248162}, 'raw': {'MSE': 18.93645978631521, 'MAE': 2.93128735386568}}}, 'encoder_infer_time': 6.251052379608154, 'lr_train_time': {24: 2.3134589195251465, 48: 2.5383174419403076, 168: 3.2391586303710938, 336: 5.604488372802734, 720: 8.689801454544067}, 'lr_infer_time': {24: 0.006848573684692383, 48: 0.009593009948730469, 168: 0.016357898712158203, 336: 0.042142391204833984, 720: 0.1010274887084961}}
Finished.
