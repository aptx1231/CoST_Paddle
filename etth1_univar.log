Dataset: ETTh1
Arguments: Namespace(dataset='ETTh1', run_name='forecast_univar', archive='forecast_csv_univar', gpu=2, batch_size=128, lr=0.001, repr_dims=320, max_train_length=201, iters=None, epochs=None, save_every=None, seed=0, max_threads=8, eval=True, kernels=[1, 2, 4, 8, 16, 32, 64, 128], alpha=0.0005)
Epoch #0: loss=0.061614859849214554
Epoch #1: loss=4.215548515319824
Epoch #2: loss=4.836483955383301
Epoch #3: loss=4.7773332595825195
Epoch #4: loss=4.692008972167969
Epoch #5: loss=4.669691562652588
Epoch #6: loss=4.604383945465088
Epoch #7: loss=4.6495680809021
Epoch #8: loss=4.539672374725342
Epoch #9: loss=4.45908784866333
Epoch #10: loss=4.526597499847412
Epoch #11: loss=4.4833502769470215
Epoch #12: loss=4.439901828765869
Epoch #13: loss=4.343565940856934
Epoch #14: loss=4.498624801635742
Epoch #15: loss=4.749454498291016
Epoch #16: loss=4.244423866271973
Epoch #17: loss=4.6986083984375
Epoch #18: loss=4.052443027496338
Epoch #19: loss=4.021710395812988
Epoch #20: loss=4.1962409019470215
Epoch #21: loss=4.324216842651367
Epoch #22: loss=4.016707897186279
Epoch #23: loss=4.150272846221924
Epoch #24: loss=4.414401531219482
Epoch #25: loss=4.23907470703125
Epoch #26: loss=4.0573296546936035
Epoch #27: loss=4.2571210861206055
Epoch #28: loss=3.9662528038024902
Epoch #29: loss=3.879880666732788
Epoch #30: loss=4.186269760131836
Epoch #31: loss=3.867910861968994
Epoch #32: loss=3.849900484085083
Epoch #33: loss=3.933305501937866
Epoch #34: loss=3.996532440185547
Epoch #35: loss=4.031950950622559
Epoch #36: loss=3.7678041458129883
Epoch #37: loss=3.8436503410339355
Epoch #38: loss=3.941077470779419
Epoch #39: loss=3.9738242626190186
Epoch #40: loss=3.834632396697998
Epoch #41: loss=4.06342887878418
Epoch #42: loss=3.398906946182251
Epoch #43: loss=3.725003242492676
Epoch #44: loss=3.7219724655151367
Epoch #45: loss=3.7663564682006836
Epoch #46: loss=3.764141798019409
Epoch #47: loss=3.769690990447998
Epoch #48: loss=4.074129104614258
Epoch #49: loss=3.1048786640167236
Epoch #50: loss=3.542884349822998
Epoch #51: loss=3.6483967304229736
Epoch #52: loss=3.637002944946289
Epoch #53: loss=3.447434186935425
Epoch #54: loss=3.479290723800659
Epoch #55: loss=3.619436502456665
Epoch #56: loss=3.4662668704986572
Epoch #57: loss=3.446742534637451
Epoch #58: loss=3.7440946102142334
Epoch #59: loss=3.472146987915039
Epoch #60: loss=3.3690924644470215
Epoch #61: loss=3.208439588546753
Epoch #62: loss=3.5358102321624756
Epoch #63: loss=3.6771795749664307
Epoch #64: loss=3.5733399391174316
Epoch #65: loss=3.5672693252563477
Epoch #66: loss=3.584188938140869
Epoch #67: loss=3.5281777381896973
Epoch #68: loss=3.5679593086242676
Epoch #69: loss=3.201798915863037
Epoch #70: loss=3.381167411804199
Epoch #71: loss=3.533111810684204
Epoch #72: loss=3.551311731338501
Epoch #73: loss=3.2389414310455322
Epoch #74: loss=3.502654790878296
Epoch #75: loss=3.780987024307251
Epoch #76: loss=2.8889272212982178
Epoch #77: loss=3.261129379272461
Epoch #78: loss=3.379384994506836
Epoch #79: loss=3.241611957550049
Epoch #80: loss=3.386605739593506
Epoch #81: loss=3.351034164428711
Epoch #82: loss=3.375917434692383
Epoch #83: loss=3.370260715484619
Epoch #84: loss=3.304719924926758
Epoch #85: loss=3.2783894538879395
Epoch #86: loss=3.4303555488586426
Epoch #87: loss=3.4780080318450928
Epoch #88: loss=3.5111753940582275
Epoch #89: loss=3.401026964187622
Epoch #90: loss=3.3196768760681152
Epoch #91: loss=3.3898532390594482
Epoch #92: loss=3.2642982006073
Epoch #93: loss=3.3113555908203125
Epoch #94: loss=3.3774197101593018
Epoch #95: loss=3.126432418823242
Epoch #96: loss=3.405578136444092
Epoch #97: loss=3.3671891689300537
Epoch #98: loss=3.2375869750976562
Epoch #99: loss=3.2225890159606934
Epoch #100: loss=3.2515411376953125
Epoch #101: loss=3.3353774547576904
Epoch #102: loss=3.4829490184783936
Epoch #103: loss=3.1276612281799316
Epoch #104: loss=3.3862249851226807
Epoch #105: loss=3.472179889678955
Epoch #106: loss=3.185509443283081
Epoch #107: loss=3.3755979537963867
Epoch #108: loss=3.0944197177886963
Epoch #109: loss=3.309286594390869
Epoch #110: loss=3.162215232849121
Epoch #111: loss=3.140552043914795
Epoch #112: loss=3.2605791091918945
Epoch #113: loss=3.1927740573883057
Epoch #114: loss=3.337322473526001
Epoch #115: loss=3.4552865028381348
Epoch #116: loss=3.2824156284332275
Epoch #117: loss=3.5661377906799316
Epoch #118: loss=3.9283409118652344
Epoch #119: loss=2.5079641342163086
Epoch #120: loss=3.4967923164367676
Epoch #121: loss=2.9752421379089355
Epoch #122: loss=3.1433401107788086
Epoch #123: loss=3.4427144527435303
Epoch #124: loss=3.2445874214172363
Epoch #125: loss=3.4736716747283936
Epoch #126: loss=2.5464909076690674
Epoch #127: loss=3.1583738327026367
Epoch #128: loss=3.298602342605591
Epoch #129: loss=3.352769374847412
Epoch #130: loss=3.3289642333984375
Epoch #131: loss=3.0759024620056152
Epoch #132: loss=2.9512248039245605
Epoch #133: loss=3.268808364868164
Epoch #134: loss=3.142071485519409
Epoch #135: loss=3.3742024898529053
Epoch #136: loss=3.2450966835021973
Epoch #137: loss=3.163302183151245
Epoch #138: loss=3.2243432998657227
Epoch #139: loss=2.796921491622925
Epoch #140: loss=2.978482961654663
Epoch #141: loss=3.1539456844329834
Epoch #142: loss=3.063385248184204
Epoch #143: loss=2.8959343433380127
Epoch #144: loss=3.277285099029541
Epoch #145: loss=2.895955801010132
Epoch #146: loss=3.0824639797210693
Epoch #147: loss=3.1579337120056152
Epoch #148: loss=3.1252825260162354
Epoch #149: loss=3.2911040782928467
Epoch #150: loss=3.068235397338867
Epoch #151: loss=2.851666212081909
Epoch #152: loss=2.815800666809082
Epoch #153: loss=2.975144147872925
Epoch #154: loss=3.116258382797241
Epoch #155: loss=3.0350215435028076
Epoch #156: loss=3.002086639404297
Epoch #157: loss=2.918525457382202
Epoch #158: loss=3.0448646545410156
Epoch #159: loss=2.7238056659698486
Epoch #160: loss=2.934530019760132
Epoch #161: loss=2.5551862716674805
Epoch #162: loss=3.118414878845215
Epoch #163: loss=2.912954092025757
Epoch #164: loss=3.0039899349212646
Epoch #165: loss=3.0904970169067383
Epoch #166: loss=2.926079273223877
Epoch #167: loss=2.9329612255096436
Epoch #168: loss=3.1967954635620117
Epoch #169: loss=2.8654351234436035
Epoch #170: loss=3.0653862953186035
Epoch #171: loss=3.347233772277832
Epoch #172: loss=2.1759119033813477
Epoch #173: loss=2.51526141166687
Epoch #174: loss=2.8071389198303223
Epoch #175: loss=2.9610342979431152
Epoch #176: loss=3.028228759765625
Epoch #177: loss=3.126143217086792
Epoch #178: loss=3.1975483894348145
Epoch #179: loss=2.563711643218994
Epoch #180: loss=2.8254172801971436
Epoch #181: loss=3.1354475021362305
Epoch #182: loss=2.9478600025177
Epoch #183: loss=3.158903121948242
Epoch #184: loss=2.8207719326019287
Epoch #185: loss=2.982452630996704
Epoch #186: loss=2.935204267501831
Epoch #187: loss=3.2035739421844482
Epoch #188: loss=2.9383907318115234
Epoch #189: loss=2.8981597423553467
Epoch #190: loss=2.824866771697998
Epoch #191: loss=2.7574403285980225
Epoch #192: loss=2.9236791133880615
Epoch #193: loss=2.8395040035247803
Epoch #194: loss=2.7468533515930176
Epoch #195: loss=2.6057653427124023
Epoch #196: loss=2.5807061195373535
Epoch #197: loss=2.801985025405884
Epoch #198: loss=3.317929267883301
Epoch #199: loss=2.1950557231903076

Training time: 0:00:55.655968

Evaluation result: {'ours': {24: {'norm': {'MSE': 0.038598027247074035, 'MAE': 0.1500648821471758}, 'raw': {'MSE': 3.250262119855083, 'MAE': 1.37706902592941}}, 48: {'norm': {'MSE': 0.05914009557617521, 'MAE': 0.1855645075389455}, 'raw': {'MSE': 4.9800683402973815, 'MAE': 1.7028310176033397}}, 168: {'norm': {'MSE': 0.09204367426207262, 'MAE': 0.23224074369381237}, 'raw': {'MSE': 7.750812435781022, 'MAE': 2.131155074794031}}, 336: {'norm': {'MSE': 0.10827163199943664, 'MAE': 0.2557067194617659}, 'raw': {'MSE': 9.117336044673241, 'MAE': 2.346490381960066}}, 720: {'norm': {'MSE': 0.12967350848771225, 'MAE': 0.28393169359773335}, 'raw': {'MSE': 10.919544934932738, 'MAE': 2.605496597618407}}}, 'encoder_infer_time': 1.971623420715332, 'lr_train_time': {24: 0.26222848892211914, 48: 0.2894771099090576, 168: 0.4121668338775635, 336: 0.5607223510742188, 720: 1.0048022270202637}, 'lr_infer_time': {24: 0.0015399456024169922, 48: 0.0016930103302001953, 168: 0.0024187564849853516, 336: 0.0033888816833496094, 720: 0.006459712982177734}}
Finished.
