Dataset: ETTh1
Arguments: Namespace(alpha=0.0005, archive='forecast_csv_univar', batch_size=128, dataset='ETTh1', epochs=None, eval=True, gpu=0, iters=None, kernels=[1, 2, 4, 8, 16, 32, 64, 128], lr=0.001, max_threads=8, max_train_length=201, repr_dims=320, run_name='forecast_univar', save_every=None, seed=0)
input_fc.weight	[8, 64]	Place(gpu:0)
input_fc.bias	[64]	Place(gpu:0)
feature_extractor.net.0.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.0.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.0.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.0.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.1.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.1.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.1.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.1.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.2.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.2.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.2.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.2.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.3.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.3.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.3.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.3.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.4.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.4.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.4.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.4.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.5.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.5.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.5.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.5.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.6.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.6.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.6.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.6.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.7.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.7.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.7.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.7.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.8.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.8.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.8.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.8.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.9.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.9.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.9.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.9.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.10.conv1.conv.weight	[320, 64, 3]	Place(gpu:0)
feature_extractor.net.10.conv1.conv.bias	[320]	Place(gpu:0)
feature_extractor.net.10.conv2.conv.weight	[320, 320, 3]	Place(gpu:0)
feature_extractor.net.10.conv2.conv.bias	[320]	Place(gpu:0)
feature_extractor.net.10.projector.weight	[320, 64, 1]	Place(gpu:0)
feature_extractor.net.10.projector.bias	[320]	Place(gpu:0)
tfd.0.weight	[160, 320, 1]	Place(gpu:0)
tfd.0.bias	[160]	Place(gpu:0)
tfd.1.weight	[160, 320, 2]	Place(gpu:0)
tfd.1.bias	[160]	Place(gpu:0)
tfd.2.weight	[160, 320, 4]	Place(gpu:0)
tfd.2.bias	[160]	Place(gpu:0)
tfd.3.weight	[160, 320, 8]	Place(gpu:0)
tfd.3.bias	[160]	Place(gpu:0)
tfd.4.weight	[160, 320, 16]	Place(gpu:0)
tfd.4.bias	[160]	Place(gpu:0)
tfd.5.weight	[160, 320, 32]	Place(gpu:0)
tfd.5.bias	[160]	Place(gpu:0)
tfd.6.weight	[160, 320, 64]	Place(gpu:0)
tfd.6.bias	[160]	Place(gpu:0)
tfd.7.weight	[160, 320, 128]	Place(gpu:0)
tfd.7.bias	[160]	Place(gpu:0)
sfd.0.weight	[101, 320, 160]	Place(gpu:0)
sfd.0.bias	[101, 160]	Place(gpu:0)
---------------------------------------------------------------
input_fc.weight	[8, 64]	Place(gpu:0)
input_fc.bias	[64]	Place(gpu:0)
feature_extractor.net.0.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.0.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.0.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.0.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.1.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.1.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.1.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.1.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.2.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.2.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.2.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.2.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.3.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.3.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.3.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.3.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.4.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.4.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.4.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.4.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.5.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.5.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.5.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.5.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.6.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.6.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.6.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.6.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.7.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.7.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.7.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.7.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.8.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.8.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.8.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.8.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.9.conv1.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.9.conv1.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.9.conv2.conv.weight	[64, 64, 3]	Place(gpu:0)
feature_extractor.net.9.conv2.conv.bias	[64]	Place(gpu:0)
feature_extractor.net.10.conv1.conv.weight	[320, 64, 3]	Place(gpu:0)
feature_extractor.net.10.conv1.conv.bias	[320]	Place(gpu:0)
feature_extractor.net.10.conv2.conv.weight	[320, 320, 3]	Place(gpu:0)
feature_extractor.net.10.conv2.conv.bias	[320]	Place(gpu:0)
feature_extractor.net.10.projector.weight	[320, 64, 1]	Place(gpu:0)
feature_extractor.net.10.projector.bias	[320]	Place(gpu:0)
tfd.0.weight	[160, 320, 1]	Place(gpu:0)
tfd.0.bias	[160]	Place(gpu:0)
tfd.1.weight	[160, 320, 2]	Place(gpu:0)
tfd.1.bias	[160]	Place(gpu:0)
tfd.2.weight	[160, 320, 4]	Place(gpu:0)
tfd.2.bias	[160]	Place(gpu:0)
tfd.3.weight	[160, 320, 8]	Place(gpu:0)
tfd.3.bias	[160]	Place(gpu:0)
tfd.4.weight	[160, 320, 16]	Place(gpu:0)
tfd.4.bias	[160]	Place(gpu:0)
tfd.5.weight	[160, 320, 32]	Place(gpu:0)
tfd.5.bias	[160]	Place(gpu:0)
tfd.6.weight	[160, 320, 64]	Place(gpu:0)
tfd.6.bias	[160]	Place(gpu:0)
tfd.7.weight	[160, 320, 128]	Place(gpu:0)
tfd.7.bias	[160]	Place(gpu:0)
sfd.0.weight	[101, 320, 160]	Place(gpu:0)
sfd.0.bias	[101, 160]	Place(gpu:0)
Epoch #0: loss=0.22184130549430847
Epoch #1: loss=2.9659409523010254
Epoch #2: loss=2.4293980598449707
Epoch #3: loss=3.390044689178467
Epoch #4: loss=3.026486396789551
Epoch #5: loss=2.66329288482666
Epoch #6: loss=0.9617456793785095
Epoch #7: loss=3.3608810901641846
Epoch #8: loss=3.176422357559204
Epoch #9: loss=3.4814352989196777
Epoch #10: loss=3.3998403549194336
Epoch #11: loss=2.5033581256866455
Epoch #12: loss=2.7933521270751953
Epoch #13: loss=3.027433156967163
Epoch #14: loss=2.834249973297119
Epoch #15: loss=3.582357168197632
Epoch #16: loss=2.4467477798461914
Epoch #17: loss=3.307340621948242
Epoch #18: loss=2.103713274002075
Epoch #19: loss=1.892483115196228
Epoch #20: loss=3.7786808013916016
Epoch #21: loss=3.636462450027466
Epoch #22: loss=2.2044260501861572
Epoch #23: loss=3.3697714805603027
Epoch #24: loss=2.171553611755371
Epoch #25: loss=3.1134116649627686
Epoch #26: loss=0.6633642315864563
Epoch #27: loss=3.417562484741211
Epoch #28: loss=2.387385845184326
Epoch #29: loss=4.312480926513672
Epoch #30: loss=3.503852128982544
Epoch #31: loss=2.0159308910369873
Epoch #32: loss=3.7151849269866943
Epoch #33: loss=2.197530508041382
Epoch #34: loss=3.039879083633423
Epoch #35: loss=2.679455518722534
Epoch #36: loss=3.3069682121276855
Epoch #37: loss=2.1518325805664062
Epoch #38: loss=3.2069456577301025
Epoch #39: loss=3.070568084716797
Epoch #40: loss=3.054687976837158
Epoch #41: loss=3.0806334018707275
Epoch #42: loss=3.2350175380706787
Epoch #43: loss=2.566358804702759
Epoch #44: loss=1.7348190546035767
Epoch #45: loss=3.236586809158325
Epoch #46: loss=2.8112447261810303
Epoch #47: loss=2.4793028831481934
Epoch #48: loss=1.2261642217636108
Epoch #49: loss=2.382946729660034
Epoch #50: loss=2.9036693572998047
Epoch #51: loss=2.839310646057129
Epoch #52: loss=1.8790833950042725
Epoch #53: loss=2.030355215072632
Epoch #54: loss=2.579500675201416
Epoch #55: loss=3.199537754058838
Epoch #56: loss=1.3754009008407593
Epoch #57: loss=2.476778507232666
Epoch #58: loss=2.5871291160583496
Epoch #59: loss=2.8706905841827393
Epoch #60: loss=1.5685744285583496
Epoch #61: loss=2.7181382179260254
Epoch #62: loss=0.8725550770759583
Epoch #63: loss=3.201645851135254
Epoch #64: loss=1.3333568572998047
Epoch #65: loss=2.4029922485351562
Epoch #66: loss=2.514286518096924
Epoch #67: loss=2.8193609714508057
Epoch #68: loss=0.8852657079696655
Epoch #69: loss=2.7583916187286377
Epoch #70: loss=2.25521183013916
Epoch #71: loss=0.9923360347747803
Epoch #72: loss=2.9191370010375977
Epoch #73: loss=3.2786102294921875
Epoch #74: loss=3.0090219974517822
Epoch #75: loss=2.954634189605713
Epoch #76: loss=2.7439777851104736
Epoch #77: loss=2.4491121768951416
Epoch #78: loss=2.1423873901367188
Epoch #79: loss=2.694819688796997
Epoch #80: loss=2.6155505180358887
Epoch #81: loss=0.7089740633964539
Epoch #82: loss=2.290651559829712
Epoch #83: loss=1.8414990901947021
Epoch #84: loss=2.8966283798217773
Epoch #85: loss=3.168029546737671
Epoch #86: loss=1.972220540046692
Epoch #87: loss=2.4913394451141357
Epoch #88: loss=0.7949129939079285
Epoch #89: loss=2.6041457653045654
Epoch #90: loss=2.7239317893981934
Epoch #91: loss=0.6242504715919495
Epoch #92: loss=2.100013494491577
Epoch #93: loss=1.8028558492660522
Epoch #94: loss=3.1902153491973877
Epoch #95: loss=2.6668174266815186
Epoch #96: loss=1.5125138759613037
Epoch #97: loss=2.6124842166900635
Epoch #98: loss=2.135302782058716
Epoch #99: loss=1.7389812469482422
Epoch #100: loss=2.1098432540893555
Epoch #101: loss=1.1690247058868408
Epoch #102: loss=2.4494121074676514
Epoch #103: loss=1.6421512365341187
Epoch #104: loss=2.0657119750976562
Epoch #105: loss=1.0731548070907593
Epoch #106: loss=2.916975736618042
Epoch #107: loss=1.7467166185379028
Epoch #108: loss=0.7901600003242493
Epoch #109: loss=2.677112102508545
Epoch #110: loss=2.067023992538452
Epoch #111: loss=1.2971612215042114
Epoch #112: loss=1.7020436525344849
Epoch #113: loss=1.8841614723205566
Epoch #114: loss=1.6621569395065308
Epoch #115: loss=2.1053481101989746
Epoch #116: loss=2.2610819339752197
Epoch #117: loss=1.2081549167633057
Epoch #118: loss=2.8199949264526367
Epoch #119: loss=1.9253805875778198
Epoch #120: loss=1.602616786956787
Epoch #121: loss=2.931478977203369
Epoch #122: loss=2.584084987640381
Epoch #123: loss=1.9043303728103638
Epoch #124: loss=0.5985924601554871
Epoch #125: loss=1.5206663608551025
Epoch #126: loss=2.018463373184204
Epoch #127: loss=2.85014271736145
Epoch #128: loss=1.7580276727676392
Epoch #129: loss=3.089160203933716
Epoch #130: loss=2.119802474975586
Epoch #131: loss=1.4627685546875
Epoch #132: loss=2.3377370834350586
Epoch #133: loss=2.378760576248169
Epoch #134: loss=2.284346342086792
Epoch #135: loss=1.222729206085205
Epoch #136: loss=2.1670241355895996
Epoch #137: loss=0.44367262721061707
Epoch #138: loss=2.677591323852539
Epoch #139: loss=2.127250909805298
Epoch #140: loss=1.1622200012207031
Epoch #141: loss=3.2654988765716553
Epoch #142: loss=2.5681591033935547
Epoch #143: loss=1.66367769241333
Epoch #144: loss=2.0150017738342285
Epoch #145: loss=1.2459796667099
Epoch #146: loss=2.1715221405029297
Epoch #147: loss=1.7680732011795044
Epoch #148: loss=2.2764556407928467
Epoch #149: loss=2.5439841747283936
Epoch #150: loss=2.181189775466919
Epoch #151: loss=0.45565131306648254
Epoch #152: loss=2.304689884185791
Epoch #153: loss=1.584012508392334
Epoch #154: loss=0.7048119902610779
Epoch #155: loss=2.445422887802124
Epoch #156: loss=0.39168843626976013
Epoch #157: loss=2.632775068283081
Epoch #158: loss=2.5758306980133057
Epoch #159: loss=2.9296743869781494
Epoch #160: loss=1.50251305103302
Epoch #161: loss=0.41036996245384216
Epoch #162: loss=2.8699424266815186
Epoch #163: loss=1.6527154445648193
Epoch #164: loss=2.6134910583496094
Epoch #165: loss=2.390169143676758
Epoch #166: loss=1.062682867050171
Epoch #167: loss=2.5139005184173584
Epoch #168: loss=2.13986873626709
Epoch #169: loss=1.0428235530853271
Epoch #170: loss=1.0526617765426636
Epoch #171: loss=2.4927589893341064
Epoch #172: loss=0.6945791244506836
Epoch #173: loss=1.7042772769927979
Epoch #174: loss=0.3336560130119324
Epoch #175: loss=1.32770574092865
Epoch #176: loss=1.8600594997406006
Epoch #177: loss=2.524496555328369
Epoch #178: loss=2.8726003170013428
Epoch #179: loss=1.290984034538269
Epoch #180: loss=2.2749993801116943
Epoch #181: loss=2.0941731929779053
Epoch #182: loss=0.8275612592697144
Epoch #183: loss=1.193423867225647
Epoch #184: loss=2.5072951316833496
Epoch #185: loss=1.5295559167861938
Epoch #186: loss=0.40235263109207153
Epoch #187: loss=0.6507502794265747
Epoch #188: loss=1.7732558250427246
Epoch #189: loss=0.31594666838645935
Epoch #190: loss=1.5852638483047485
Epoch #191: loss=0.8797599673271179
Epoch #192: loss=1.7508283853530884
Epoch #193: loss=2.6506521701812744
Epoch #194: loss=0.6886364221572876
Epoch #195: loss=1.2989459037780762
Epoch #196: loss=2.04575514793396
Epoch #197: loss=0.3836866319179535
Epoch #198: loss=1.905300259590149
Epoch #199: loss=2.8489205837249756

Training time: 0:02:18.313145

Evaluation result: {'ours': {24: {'norm': {'MSE': 0.13755247418438613, 'MAE': 0.3025568372630871}, 'raw': {'MSE': 11.583016880391005, 'MAE': 2.7764100707308685}}, 48: {'norm': {'MSE': 0.17152725969244403, 'MAE': 0.33710979502905425}, 'raw': {'MSE': 14.44396518453671, 'MAE': 3.093484978473784}}, 168: {'norm': {'MSE': 0.22181863792675843, 'MAE': 0.3869780573520121}, 'raw': {'MSE': 18.678900941331502, 'MAE': 3.5511006427287466}}, 336: {'norm': {'MSE': 0.22969026683751115, 'MAE': 0.397051287167967}, 'raw': {'MSE': 19.34175495152965, 'MAE': 3.6435375441791584}}, 720: {'norm': {'MSE': 0.2440294517191408, 'MAE': 0.412534617482083}, 'raw': {'MSE': 20.549228866296982, 'MAE': 3.7856201849398814}}}, 'encoder_infer_time': 6.927388668060303, 'lr_train_time': {24: 1.2008280754089355, 48: 0.8898441791534424, 168: 1.4425327777862549, 336: 2.3614213466644287, 720: 3.168459415435791}, 'lr_infer_time': {24: 0.012161731719970703, 48: 0.00639033317565918, 168: 0.04725384712219238, 336: 0.010668516159057617, 720: 0.017612218856811523}}
Finished.
