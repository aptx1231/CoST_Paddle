Dataset: ETTh2
Arguments: Namespace(alpha=0.0005, archive='forecast_csv_univar', batch_size=128, dataset='ETTh2', epochs=None, eval=True, gpu=1, iters=None, kernels=[1, 2, 4, 8, 16, 32, 64, 128], lr=0.001, max_threads=8, max_train_length=201, repr_dims=320, run_name='forecast_univar', save_every=None, seed=0)
input_fc.weight	[8, 64]	Place(gpu:1)
input_fc.bias	[64]	Place(gpu:1)
feature_extractor.net.0.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.0.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.0.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.0.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.1.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.1.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.1.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.1.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.2.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.2.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.2.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.2.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.3.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.3.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.3.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.3.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.4.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.4.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.4.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.4.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.5.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.5.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.5.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.5.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.6.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.6.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.6.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.6.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.7.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.7.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.7.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.7.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.8.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.8.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.8.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.8.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.9.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.9.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.9.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.9.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.10.conv1.conv.weight	[320, 64, 3]	Place(gpu:1)
feature_extractor.net.10.conv1.conv.bias	[320]	Place(gpu:1)
feature_extractor.net.10.conv2.conv.weight	[320, 320, 3]	Place(gpu:1)
feature_extractor.net.10.conv2.conv.bias	[320]	Place(gpu:1)
feature_extractor.net.10.projector.weight	[320, 64, 1]	Place(gpu:1)
feature_extractor.net.10.projector.bias	[320]	Place(gpu:1)
tfd.0.weight	[160, 320, 1]	Place(gpu:1)
tfd.0.bias	[160]	Place(gpu:1)
tfd.1.weight	[160, 320, 2]	Place(gpu:1)
tfd.1.bias	[160]	Place(gpu:1)
tfd.2.weight	[160, 320, 4]	Place(gpu:1)
tfd.2.bias	[160]	Place(gpu:1)
tfd.3.weight	[160, 320, 8]	Place(gpu:1)
tfd.3.bias	[160]	Place(gpu:1)
tfd.4.weight	[160, 320, 16]	Place(gpu:1)
tfd.4.bias	[160]	Place(gpu:1)
tfd.5.weight	[160, 320, 32]	Place(gpu:1)
tfd.5.bias	[160]	Place(gpu:1)
tfd.6.weight	[160, 320, 64]	Place(gpu:1)
tfd.6.bias	[160]	Place(gpu:1)
tfd.7.weight	[160, 320, 128]	Place(gpu:1)
tfd.7.bias	[160]	Place(gpu:1)
sfd.0.weight	[101, 320, 160]	Place(gpu:1)
sfd.0.bias	[101, 160]	Place(gpu:1)
---------------------------------------------------------------
input_fc.weight	[8, 64]	Place(gpu:1)
input_fc.bias	[64]	Place(gpu:1)
feature_extractor.net.0.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.0.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.0.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.0.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.1.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.1.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.1.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.1.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.2.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.2.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.2.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.2.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.3.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.3.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.3.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.3.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.4.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.4.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.4.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.4.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.5.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.5.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.5.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.5.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.6.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.6.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.6.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.6.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.7.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.7.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.7.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.7.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.8.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.8.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.8.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.8.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.9.conv1.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.9.conv1.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.9.conv2.conv.weight	[64, 64, 3]	Place(gpu:1)
feature_extractor.net.9.conv2.conv.bias	[64]	Place(gpu:1)
feature_extractor.net.10.conv1.conv.weight	[320, 64, 3]	Place(gpu:1)
feature_extractor.net.10.conv1.conv.bias	[320]	Place(gpu:1)
feature_extractor.net.10.conv2.conv.weight	[320, 320, 3]	Place(gpu:1)
feature_extractor.net.10.conv2.conv.bias	[320]	Place(gpu:1)
feature_extractor.net.10.projector.weight	[320, 64, 1]	Place(gpu:1)
feature_extractor.net.10.projector.bias	[320]	Place(gpu:1)
tfd.0.weight	[160, 320, 1]	Place(gpu:1)
tfd.0.bias	[160]	Place(gpu:1)
tfd.1.weight	[160, 320, 2]	Place(gpu:1)
tfd.1.bias	[160]	Place(gpu:1)
tfd.2.weight	[160, 320, 4]	Place(gpu:1)
tfd.2.bias	[160]	Place(gpu:1)
tfd.3.weight	[160, 320, 8]	Place(gpu:1)
tfd.3.bias	[160]	Place(gpu:1)
tfd.4.weight	[160, 320, 16]	Place(gpu:1)
tfd.4.bias	[160]	Place(gpu:1)
tfd.5.weight	[160, 320, 32]	Place(gpu:1)
tfd.5.bias	[160]	Place(gpu:1)
tfd.6.weight	[160, 320, 64]	Place(gpu:1)
tfd.6.bias	[160]	Place(gpu:1)
tfd.7.weight	[160, 320, 128]	Place(gpu:1)
tfd.7.bias	[160]	Place(gpu:1)
sfd.0.weight	[101, 320, 160]	Place(gpu:1)
sfd.0.bias	[101, 160]	Place(gpu:1)
Epoch #0: loss=0.20418384671211243
Epoch #1: loss=2.943157911300659
Epoch #2: loss=2.371222496032715
Epoch #3: loss=3.376399517059326
Epoch #4: loss=2.981973648071289
Epoch #5: loss=2.6130735874176025
Epoch #6: loss=0.962944507598877
Epoch #7: loss=3.300339698791504
Epoch #8: loss=3.0836074352264404
Epoch #9: loss=3.5468952655792236
Epoch #10: loss=3.3621113300323486
Epoch #11: loss=2.498793840408325
Epoch #12: loss=2.7490038871765137
Epoch #13: loss=3.123849391937256
Epoch #14: loss=2.87176775932312
Epoch #15: loss=3.6731081008911133
Epoch #16: loss=2.4839673042297363
Epoch #17: loss=3.2196500301361084
Epoch #18: loss=2.1350510120391846
Epoch #19: loss=1.9837356805801392
Epoch #20: loss=3.761504650115967
Epoch #21: loss=3.6810379028320312
Epoch #22: loss=2.309875249862671
Epoch #23: loss=3.2947990894317627
Epoch #24: loss=2.1602516174316406
Epoch #25: loss=3.104529619216919
Epoch #26: loss=0.6055240035057068
Epoch #27: loss=3.5618948936462402
Epoch #28: loss=2.40116548538208
Epoch #29: loss=4.444232940673828
Epoch #30: loss=3.549774169921875
Epoch #31: loss=1.8910444974899292
Epoch #32: loss=3.6461093425750732
Epoch #33: loss=2.248314380645752
Epoch #34: loss=3.012098789215088
Epoch #35: loss=2.60613751411438
Epoch #36: loss=3.2586288452148438
Epoch #37: loss=2.0925867557525635
Epoch #38: loss=3.234938621520996
Epoch #39: loss=2.910313844680786
Epoch #40: loss=3.0730221271514893
Epoch #41: loss=3.0904598236083984
Epoch #42: loss=3.2829809188842773
Epoch #43: loss=2.5696215629577637
Epoch #44: loss=1.611114263534546
Epoch #45: loss=3.170053482055664
Epoch #46: loss=2.9632043838500977
Epoch #47: loss=2.5200462341308594
Epoch #48: loss=1.1211748123168945
Epoch #49: loss=2.3699169158935547
Epoch #50: loss=2.932426929473877
Epoch #51: loss=2.8395133018493652
Epoch #52: loss=1.8799850940704346
Epoch #53: loss=2.049393892288208
Epoch #54: loss=2.536595344543457
Epoch #55: loss=3.1694114208221436
Epoch #56: loss=1.4196760654449463
Epoch #57: loss=2.394650459289551
Epoch #58: loss=2.609520435333252
Epoch #59: loss=2.778968095779419
Epoch #60: loss=1.5607126951217651
Epoch #61: loss=2.6493914127349854
Epoch #62: loss=0.8446370363235474
Epoch #63: loss=3.142601728439331
Epoch #64: loss=1.2388768196105957
Epoch #65: loss=2.2982728481292725
Epoch #66: loss=2.6679749488830566
Epoch #67: loss=2.795043468475342
Epoch #68: loss=0.819445788860321
Epoch #69: loss=2.7579610347747803
Epoch #70: loss=2.2433273792266846
Epoch #71: loss=0.9193850159645081
Epoch #72: loss=2.8210086822509766
Epoch #73: loss=3.278557538986206
Epoch #74: loss=3.0116915702819824
Epoch #75: loss=2.8826730251312256
Epoch #76: loss=2.7437446117401123
Epoch #77: loss=2.3324007987976074
Epoch #78: loss=2.0847151279449463
Epoch #79: loss=2.6702146530151367
Epoch #80: loss=2.6145455837249756
Epoch #81: loss=0.6251510381698608
Epoch #82: loss=2.279423713684082
Epoch #83: loss=1.92353355884552
Epoch #84: loss=2.9868690967559814
Epoch #85: loss=3.1478703022003174
Epoch #86: loss=1.9415507316589355
Epoch #87: loss=2.4454257488250732
Epoch #88: loss=0.7546956539154053
Epoch #89: loss=2.56896710395813
Epoch #90: loss=2.5824761390686035
Epoch #91: loss=0.5734882354736328
Epoch #92: loss=2.029658794403076
Epoch #93: loss=1.7714074850082397
Epoch #94: loss=3.2841060161590576
Epoch #95: loss=2.71702241897583
Epoch #96: loss=1.4870151281356812
Epoch #97: loss=2.6145429611206055
Epoch #98: loss=2.0446975231170654
Epoch #99: loss=1.7003728151321411
Epoch #100: loss=2.06209135055542
Epoch #101: loss=1.1417217254638672
Epoch #102: loss=2.3196542263031006
Epoch #103: loss=1.6297086477279663
Epoch #104: loss=2.020998001098633
Epoch #105: loss=0.9758644700050354
Epoch #106: loss=2.9550933837890625
Epoch #107: loss=1.7001043558120728
Epoch #108: loss=0.7771037817001343
Epoch #109: loss=2.669123649597168
Epoch #110: loss=2.088006019592285
Epoch #111: loss=1.2748525142669678
Epoch #112: loss=1.6424275636672974
Epoch #113: loss=1.9212970733642578
Epoch #114: loss=1.58132803440094
Epoch #115: loss=2.082595109939575
Epoch #116: loss=2.1747498512268066
Epoch #117: loss=1.1304576396942139
Epoch #118: loss=2.7718424797058105
Epoch #119: loss=1.8927974700927734
Epoch #120: loss=1.620588779449463
Epoch #121: loss=2.8802361488342285
Epoch #122: loss=2.585461378097534
Epoch #123: loss=1.785271167755127
Epoch #124: loss=0.5713196396827698
Epoch #125: loss=1.504315733909607
Epoch #126: loss=2.0230937004089355
Epoch #127: loss=2.8006393909454346
Epoch #128: loss=1.8178274631500244
Epoch #129: loss=2.9805753231048584
Epoch #130: loss=2.0572350025177
Epoch #131: loss=1.3865630626678467
Epoch #132: loss=2.3386449813842773
Epoch #133: loss=2.293241262435913
Epoch #134: loss=2.2528631687164307
Epoch #135: loss=1.1162786483764648
Epoch #136: loss=2.1647448539733887
Epoch #137: loss=0.5178870558738708
Epoch #138: loss=2.711785316467285
Epoch #139: loss=2.1125802993774414
Epoch #140: loss=1.1592220067977905
Epoch #141: loss=3.276087999343872
Epoch #142: loss=2.522536039352417
Epoch #143: loss=1.580472707748413
Epoch #144: loss=1.943504810333252
Epoch #145: loss=1.1867825984954834
Epoch #146: loss=2.2562596797943115
Epoch #147: loss=1.6170625686645508
Epoch #148: loss=2.305131196975708
Epoch #149: loss=2.4812252521514893
Epoch #150: loss=2.1867239475250244
Epoch #151: loss=0.3950554132461548
Epoch #152: loss=2.385307550430298
Epoch #153: loss=1.5538800954818726
Epoch #154: loss=0.7132521271705627
Epoch #155: loss=2.430231809616089
Epoch #156: loss=0.3407934308052063
Epoch #157: loss=2.6339800357818604
Epoch #158: loss=2.489940881729126
Epoch #159: loss=2.934980869293213
Epoch #160: loss=1.5582016706466675
Epoch #161: loss=0.3807498812675476
Epoch #162: loss=2.8093018531799316
Epoch #163: loss=1.602609395980835
Epoch #164: loss=2.5809738636016846
Epoch #165: loss=2.373873710632324
Epoch #166: loss=1.038045048713684
Epoch #167: loss=2.501788377761841
Epoch #168: loss=2.0272834300994873
Epoch #169: loss=1.0584204196929932
Epoch #170: loss=1.038060188293457
Epoch #171: loss=2.4270379543304443
Epoch #172: loss=0.6234082579612732
Epoch #173: loss=1.7170562744140625
Epoch #174: loss=0.27015867829322815
Epoch #175: loss=1.267322063446045
Epoch #176: loss=1.8652273416519165
Epoch #177: loss=2.562288284301758
Epoch #178: loss=2.8190245628356934
Epoch #179: loss=1.3262224197387695
Epoch #180: loss=2.249882698059082
Epoch #181: loss=1.9973068237304688
Epoch #182: loss=0.8319118618965149
Epoch #183: loss=1.1878362894058228
Epoch #184: loss=2.350025177001953
Epoch #185: loss=1.532974362373352
Epoch #186: loss=0.36130914092063904
Epoch #187: loss=0.5904600620269775
Epoch #188: loss=1.696150541305542
Epoch #189: loss=0.3241022527217865
Epoch #190: loss=1.4965144395828247
Epoch #191: loss=0.8580285310745239
Epoch #192: loss=1.756052017211914
Epoch #193: loss=2.5599939823150635
Epoch #194: loss=0.5729049444198608
Epoch #195: loss=1.3169872760772705
Epoch #196: loss=1.9951163530349731
Epoch #197: loss=0.35170844197273254
Epoch #198: loss=1.8795063495635986
Epoch #199: loss=2.8151304721832275

Training time: 0:01:30.398790

Evaluation result: {'ours': {24: {'norm': {'MSE': 0.1200310567961676, 'MAE': 0.27349229685598125}, 'raw': {'MSE': 16.108853399628103, 'MAE': 3.1683313772968456}}, 48: {'norm': {'MSE': 0.15145100394684202, 'MAE': 0.30777762825388727}, 'raw': {'MSE': 20.325589732092443, 'MAE': 3.5655172994006885}}, 168: {'norm': {'MSE': 0.20245042763730256, 'MAE': 0.35876398643497465}, 'raw': {'MSE': 27.170003644353596, 'MAE': 4.156179925291597}}, 336: {'norm': {'MSE': 0.2206736741084011, 'MAE': 0.37920305402806537}, 'raw': {'MSE': 29.61566735775684, 'MAE': 4.392960774796681}}, 720: {'norm': {'MSE': 0.22786237771850565, 'MAE': 0.3887894060442765}, 'raw': {'MSE': 30.580432314802433, 'MAE': 4.504015959617548}}}, 'encoder_infer_time': 4.342660427093506, 'lr_train_time': {24: 1.6274449825286865, 48: 1.408994436264038, 168: 1.9323947429656982, 336: 2.0522658824920654, 720: 2.716524362564087}, 'lr_infer_time': {24: 0.007028818130493164, 48: 0.007542133331298828, 168: 0.01602792739868164, 336: 0.015492439270019531, 720: 0.04732966423034668}}
Finished.
